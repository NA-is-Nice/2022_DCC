{"cells":[{"cell_type":"code","execution_count":null,"id":"f899bcec","metadata":{"lines_to_next_cell":2,"id":"f899bcec"},"outputs":[],"source":["import os\n","from random import shuffle\n","import time\n","import json\n","import torch\n","import hashlib\n","import argparse\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"id":"90646707","metadata":{"lines_to_next_cell":2,"id":"90646707"},"outputs":[],"source":["from os import listdir\n","from torch import optim\n","from copy import deepcopy\n","from os.path import isfile, join\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from torch.utils.data import WeightedRandomSampler"]},{"cell_type":"code","execution_count":null,"id":"079ac10d","metadata":{"id":"079ac10d"},"outputs":[],"source":["# Preliminaries\n","TRAIN_DATA_DIR = './JYsplit/train'\n","VAL_DATA_DIR = './JYsplit/val'\n","TEST_DATA_DIR = './JYsplit/test'"]},{"cell_type":"code","execution_count":null,"id":"007dcc6b","metadata":{"id":"007dcc6b"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"id":"852d85c3","metadata":{"id":"852d85c3"},"outputs":[],"source":["# Preprocess data\n","transform_train = transforms.Compose([\n","    transforms.Resize(256), \n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","])"]},{"cell_type":"code","execution_count":null,"id":"0c5fc28d","metadata":{"id":"0c5fc28d"},"outputs":[],"source":["assert os.path.exists(TRAIN_DATA_DIR)\n","train_dataset = ImageFolder(root=TRAIN_DATA_DIR, transform=transform_train)\n","val_dataset = ImageFolder(root=VAL_DATA_DIR, transform=transform_train)\n","test_dataset = ImageFolder(root=TEST_DATA_DIR, transform=transform_train)\n","\n","print(train_dataset.class_to_idx)\n","\n","partition = {'train':train_dataset, 'val':val_dataset, 'test':test_dataset}"]},{"cell_type":"code","execution_count":null,"id":"749edafc","metadata":{"id":"749edafc"},"outputs":[],"source":["# plt.figure(figsize=(15,8))\n","# sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(train_dataset)]).melt(),\n","#             x = \"variable\", y=\"value\", hue=\"variable\").set_title('Images Class Distribution')"]},{"cell_type":"code","execution_count":null,"id":"5c6d2843","metadata":{"id":"5c6d2843"},"outputs":[],"source":["idx2class = {v: k for k, v in train_dataset.class_to_idx.items()}\n","\n","def get_class_distribution(dataset_obj):\n","    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n","    \n","    for element in dataset_obj:\n","        y_lbl = element[1]\n","        y_lbl = idx2class[y_lbl]\n","        count_dict[y_lbl] += 1\n","            \n","    return count_dict\n","\n","print(\"Distribution of classes: \\n\", get_class_distribution(train_dataset))"]},{"cell_type":"code","execution_count":null,"id":"22b88f5c","metadata":{"id":"22b88f5c"},"outputs":[],"source":["target_list = torch.tensor(train_dataset.targets)\n","\n","class_count = [i for i in get_class_distribution(train_dataset).values()]\n","class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n","class_weights"]},{"cell_type":"code","execution_count":null,"id":"2ce6afd4","metadata":{"id":"2ce6afd4"},"outputs":[],"source":["class_weights_all = class_weights[target_list]\n","class_weights_all"]},{"cell_type":"code","execution_count":null,"id":"ce046111","metadata":{"id":"ce046111"},"outputs":[],"source":["weighted_sampler = WeightedRandomSampler(\n","    weights=class_weights_all,\n","    num_samples=len(class_weights_all),\n","    replacement=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"8da579a4","metadata":{"lines_to_next_cell":1,"id":"8da579a4"},"outputs":[],"source":["# VGG16 Model \n","class VGG16(nn.Module):\n","    def __init__(self, num_classes, in_channels, out_channels, act, dropout, use_bn):\n","        super(VGG16, self).__init__()\n","        self.num_classes = num_classes\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.act = act\n","        self.dropout = dropout\n","        self.use_bn = use_bn\n","\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(num_features = 64),\n","            nn.ReLU()\n","        )\n","\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(num_features = 64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )\n","\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU()\n","        )\n","\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(128, 128, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )\n","\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","\n","        self.layer6 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","\n","        self.layer7 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        ) \n","\n","        self.layer8 = nn.Sequential(\n","            nn.Conv2d(256, 512, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","\n","        self.layer9 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","\n","        self.layer10 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )            \n","        \n","        self.layer11 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","\n","        self.layer12 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","\n","        self.layer13 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )            \n","\n","        self.fc1 = nn.Sequential(\n","            nn.Dropout(self.dropout),\n","            nn.Linear(512*7*7, 4096),\n","            nn.ReLU()\n","        )\n","\n","        self.fc2 = nn.Sequential(\n","            nn.Dropout(self.dropout),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU()\n","        )\n","\n","        self.fc3 = nn.Sequential(\n","            nn.Linear(4096, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.layer5(out)\n","        out = self.layer6(out)\n","        out = self.layer7(out)\n","        out = self.layer8(out)\n","        out = self.layer9(out)\n","        out = self.layer10(out)\n","        out = self.layer11(out)    \n","        out = self.layer12(out)\n","        out = self.layer13(out)\n","        out = out.view(out.size(0), -1) # 배치사이즈 dimension은 유지하고 나머지 부분은 쭉 펴진 것\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        out = self.fc3(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"id":"800c78c1","metadata":{"lines_to_next_cell":1,"id":"800c78c1"},"outputs":[],"source":["# Train function \n","def train(model, partition, optimizer, criterion, args):\n","    train_loader = torch.utils.data.DataLoader(\n","        partition['train'],\n","        batch_size = args.train_batch_size,\n","        sampler = weighted_sampler,\n","        shuffle = False,\n","        num_workers = 2)\n","    \n","    model.train()\n","    correct = 0\n","    total = 0\n","    train_loss = 0.0\n","    \n","    for i, (inputs, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    train_loss = train_loss / len(train_loader)\n","    train_acc = 100 * correct / total\n","\n","    # model 반환 안해주면 학습시킨 model이 아닌 아무것도 안한 처음 모델로 validation 학습시키는 것\n","    return model, train_loss, train_acc"]},{"cell_type":"code","execution_count":null,"id":"57a23b6f","metadata":{"lines_to_next_cell":1,"id":"57a23b6f"},"outputs":[],"source":["# Validation function\n","def validate(model, partition, criterion, args):\n","    val_loader = torch.utils.data.DataLoader(\n","                    partition['val'],\n","                    batch_size = args.test_batch_size,\n","                    shuffle = False,\n","                    num_workers = 2)\n","    \n","    model.eval()\n","    \n","    correct = 0\n","    total = 0\n","    val_loss = 0.0\n","    \n","    with torch.no_grad():\n","        for i, (images, labels) in enumerate(val_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","\n","            loss = criterion(outputs, labels)\n","\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(val_loader)\n","        val_acc = 100 * correct / total\n","\n","    return val_loss, val_acc"]},{"cell_type":"code","execution_count":null,"id":"07c97ffa","metadata":{"lines_to_next_cell":1,"id":"07c97ffa"},"outputs":[],"source":["# Test function\n","def test(model, partition, args):\n","    test_loader = torch.utils.data.DataLoader(partition['test'],\n","                                                batch_size = args.test_batch_size,\n","                                                shuffle = False,\n","                                                num_workers = 2)\n","    \n","    model.eval()\n","    \n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for i, (images, labels) in enumerate(test_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        test_acc = 100 * correct / total\n","\n","    return test_acc"]},{"cell_type":"code","execution_count":null,"id":"8c780bd2","metadata":{"id":"8c780bd2"},"outputs":[],"source":["# Experiment function\n","def experiment(partition, args):\n","    model = VGG16(num_classes = args.num_classes, \n","                    in_channels = args.in_channels, \n","                    out_channels = args.out_channels,\n","                    act = args.act,\n","                    dropout = args.dropout,\n","                    use_bn = args.use_bn)\n","    model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    if args.optim == 'SGD':\n","        optimizer = optim.SGD(model.parameters(), lr = args.lr, weight_decay = args.l2)\n","    elif args.optim == 'RMSprop':\n","        optimizer = optim.RMSprop(model.parameters(), lr = args.lr, weight_decay = args.l2)\n","    elif args.optim == 'Adam':\n","        optimizer = optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.l2)\n","    elif args.optim == 'AdamW':\n","        optimizer = optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.l2)\n","    else:\n","        raise ValueError('In-valid optimizer choice')\n","\n","    # =========== List for epoch-wise data ============= #\n","    train_losses = []\n","    val_losses = []\n","    train_accs = []\n","    val_accs = []\n","\n","    # =========== Experiment ============= #\n","    for epoch in range(args.num_epochs):\n","        ts = time.time()\n","        model, train_loss, train_acc = train(model, partition, optimizer, criterion, args)\n","        val_loss, val_acc = validate(model, partition, criterion, args)\n","        te = time.time()\n","\n","        # ========== Add Epoch Data ============ #\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        train_accs.append(train_acc)\n","        val_accs.append(val_acc)\n","\n","\n","        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch+1, train_acc, val_acc, train_loss, val_loss, te-ts))\n","        \n","        # save model for each 2 epochs\n","        if (epoch+1) % 2 == 0:\n","            os.makedirs(f'./models/weightedsampler', exist_ok=True)\n","            torch.save(model.state_dict(),\n","                       f'./models/weightedsampler/weighted_{epoch+1}.ckpt')\n","        \n","    test_acc = test(model, partition, args)\n","\n","    # ======== Add Result to Dictionary ========== #\n","    result = {}\n","    result['train_losses'] = train_losses\n","    result['val_losses'] = val_losses\n","    result['train_accs'] = train_accs\n","    result['val_accs'] = val_accs\n","    result['train_acc'] = train_acc\n","    result['val_acc'] = val_acc\n","    result['test_acc'] = test_acc\n","\n","    # vars로 감싸면 dictionary가 됨\n","    return vars(args), result"]},{"cell_type":"code","execution_count":null,"id":"6466ea64","metadata":{"id":"6466ea64"},"outputs":[],"source":["def save_exp_result(setting, result):\n","    exp_name = setting['exp_name']\n","    del setting['num_epochs']\n","    del setting['test_batch_size']\n","\n","#    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n","#    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n","    filename = f'./results/weighted_{args.num_epochs}.json'\n","    result.update(setting) # dictionary와 dictionary 합치기\n","    with open(filename, 'w') as f:\n","        json.dump(result, f)\n","        \n","def load_exp_result(exp_name):\n","    dir_path = './results'\n","    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n","    list_result = []\n","    for filename in filenames:\n","        if exp_name in filename:\n","            with open(join(dir_path, filename), 'r') as infile:\n","                results = json.load(infile)\n","                list_result.append(results)\n","    df = pd.DataFrame(list_result) # .drop(columns=[])\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"798cfaeb","metadata":{"lines_to_next_cell":0,"id":"798cfaeb"},"outputs":[],"source":["# ======== Random Seed Initialization ======= #\n","seed = 0\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","parser = argparse.ArgumentParser()\n","args = parser.parse_args(\"\")\n","args.exp_name = \"bs_64_BN_AdamW_lr_0.0001_vgg16\"\n","\n","# ========= Model Capacity ============= #\n","args.in_channels = 3\n","args.out_channels = 64\n","args.num_groups = 4\n","args.act = 'relu'\n","\n","# ========== Regularization ============ #\n","args.dropout = 0.5\n","args.use_bn = True\n","args.l2 = 0.00001\n","\n","# ========== Optimizer & Training ========= #\n","args.optim = 'AdamW'\n","args.num_classes = 20\n","args.lr = 0.0001\n","args.num_epochs = 70\n","\n","args.train_batch_size = 64\n","args.test_batch_size = 32\n","\n","# ========= Experiment Variable ========== #\n","name_var1 = 'num_epochs'\n","name_var2 = 'lr'\n","list_var1 = [args.num_epochs]\n","list_var2 = [args.lr]\n","\n","for var1 in list_var1:\n","  for var2 in list_var2:\n","    setattr(args, name_var1, var1) # args.name_var1 = var1 이라는 문법과 동일\n","    setattr(args, name_var2, var2)\n","    print(args)\n","\n","    setting, result = experiment(partition, deepcopy(args))\n","    save_exp_result(setting, result)"]},{"cell_type":"code","execution_count":null,"id":"68a8d04a","metadata":{"lines_to_next_cell":2,"id":"68a8d04a"},"outputs":[],"source":[]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","encoding":"# -*- coding: utf-8 -*-","main_language":"python","notebook_metadata_filter":"-all","text_representation":{"extension":".py","format_name":"light"}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}